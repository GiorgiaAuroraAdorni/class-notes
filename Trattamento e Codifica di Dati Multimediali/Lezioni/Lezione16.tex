\section{Compressione video}
La compressione video deve necessariamente essere lossy, dato che la quantità di ridondanze è generalmente elevata e le dimensioni dei video non compresse sono ingestibili dai supporti di memoria. Il tipo di processing adottato è intra-frame o inter-frame. 

Ci sono soluzioni che considerano solo la ridondanza spaziale o temporale, e altre che sfruttano la ridondanza percettiva. 

Il procedimento più semplice applica codifiche predittive frame per frame oppure comprime usando JPEG, ma queste tecniche sono legate solo alle caratteristiche spaziali: esse devono essere estese valutando anche la ridondanza temporale, in termini di differenze tra i frame con predizioni (istogramma piccato).

Il primo step è naturalmente la differenza tra frame consecutivi: questo funziona in modo ottimale quando le zone delle immagini cambiano poco, ma non è efficace quando la camera non è perfettamente ferma, o ci sono cambi di luce, o le scene sono in movimento.

Generalmente anche in presenza di queste condizioni c'è correlazione, ma se la differenza tra due fotogrammi è il rumore o è dovuta al cambio di scena il segnale è poco correlato. In alcuni casi è possibile predirre anche il movimento.

La motion compensation è una codifica predittiva che consiste nella stima dei vettori di moto con cui le porzioni di frame vengono spostati, e nella loro applicazione per le variazioni future. La stima riguarda gli spostamenti dei macro blocchi 16$\times$16.

Gli I-frame (intra/independent frame, compressi con JPEG) sono la partenza per la predizione, mentre i risultati sono i P-frame. Essi si distinguono tra:
\begin{itemize}
	\item Predetti solo in base al reference frame;
	\item Predetti in base al frame precedente e ai frame successivi. 
\end{itemize}
Utilizzare le informazioni dei frame consecutivi aumenta la precisione.

Ogni gruppo non sovrapposto di pixel viene assegnato a un vettore di moto comune. L'intensità è 16, con 8 canali cromatici ($\nicefrac{1}{4}$) per semplificare il campionamento. 

Dato un macroblocco del target frame, viene cercato il più simile macroblocco nel reference frame tra i precedenti e successivi. Le dimensioni della finestra di ricerca variano in base allo standard utilizzato. 

% cose

Le possibilità sono la codifica della differenza oppure la compensazione della differenza attraverso il moto. 

\begin{enumerate}
	\item Individuazione del target frame;
	\item Scorrimento dei macroblocchi e ricerca del più simile al blocco del reference frame;
	\item Sostituzione con il più simile traslato di un vettore di moto;
	\item Calcolo della differenza tra predizione e target;
	\item Codifica della differenza.
\end{enumerate}

I blocchi del target frame sono organizzati su una griglia ...

La compensazione del moto viene applicata in modo efficiente ai canali di luminanza, mentre i valori di crominanza sono meno correlati. 

Se il moto è da sinistra verso destra, le zone di sinistra hanno l'errore di predizione più alto perché sono zone che non esistono nei frame precedenti.

La stima del moto con frame successivi implica un ritardo nei calcoli finché le informazioni sono incomplete.

Per semplificare le operazioni, la ricerca del macroblocco è effettuata in un intorno. 
\begin{itemize}
	\item Ricerca logaritmica, valutando la MAD su 9 punti (3$\times$3), selezionando il più vicino e dividendolo a sua volta, fino ad arrivare a un errore tollerabile, nonostante il rischio di concentrazione sui punti locali tralasciando il possibile ottimo globale;
	\item Multirisoluzione, in tre passi;
	\begin{enumerate}
		\item Stima iniziale sul frame a risoluzione minore;
		\item Raffinazione a risoluzione intermedia;
		\item Determinazione del vettore di moto finale sulla risoluzione massima.
	\end{enumerate}
\end{itemize}

Gli standard di codifica video digitale sono regolati da:
\begin{itemize}
	\item ITU (raccomandazioni), con un carattere generale e orientato alle applicazioni real-time (H.xx);
	\item ISO, 
\end{itemize}


Gli IPB frames sono particolari sequenze di frame, la cui combinazione forma il GOP:
\begin{itemize}
	\item I servono per predirre i frame successivi;
	\item B (backwards), ;
	\item P (picture), 
\end{itemize}

Il rapporto di compressione è più alto perché questa metodologia non sfrutta solo la ridondanza spaziale, ma riduce anche quella temporale. I frame B sono quelli su cui la perdita è maggiore, perché vengono codificati agendo su entrambe le direzioni.

Non sempre i macroblocchi sono codificati nello stesso modo: a seconda del tipo di frame e dell'errore di predizione essi saranno determinati diversamente. Se nessuna strategia è la migliore, si utilizza l'interpolazione.

MPEG-2 è un'evoluzione di MPEG-1, con la stessa struttura base ma dedicato a video di qualità superiore, con bitrate maggiori di 4 Mbps. Questo standard implementa diversi standard di adattabilità con livelli di decodifica in base ai frame rate, pertanto è più scalabile.

Le versioni sono a qualità variabile, trattando i coefficienti secondo la perdita indicata. I video sono interlacciati, gestendo i field (righe dispari e pari), ottenendo un errore più basso.

L'utilizzo dei field ha un effetto diretto sulla quantizzazione: il campo è 8$\times$16, il pattern a zig-zag è opzionale e sono ammessi anche pattern alternativi. Il sottocampionamento è solo in una dimensione, essendo i sottoinsiemi incorrelati. 

Non tutti i livelli ammettono le stesse variazioni: esse cambiano in base alla dimensione e i frame al secondo, campionando con diversi profili di chroma. 

Il concetto di scalabilità prevede alcune caratteristiche:
\begin{itemize}
	\item Livelli base a qualità inferore;
\end{itemize}

I layers sono definiti in base ad alcune possibilità, migliorando SNR, frequenze, frame rate o risoluzione spaziale. Questi modi possono eventualmente essere combinati in una scalabilità ibrida. 

MPEG-4 pone l'attenzione sull'interattività con l'utente, introducendo ulteriori caratteristiche basate sugli oggetti e le loro relazioni.

Esistono anche possibili oggetti precodificati, come le animazioni che permettono di inserire effetti speciali nei video.

Alcuni standard di decompressione introducono un filtro di denoising, applicato dopo la decodifica per rimuovere gli artefatti di blocchettizzazione. 

MPEG-7 definisce gli strumenti necessari per descrivere il contenuto audiovisuale dei dati multimediali e consentire l'interoperabilità tra diversi sistemi e applicazioni.

Ogni scena viene caratterizzata e classificata secondo un numero di features che impongono un grado di correlazione tra gli oggetti.


\section{Formati immagine}

\subsection{Bitmap}
Bitmap è il formato più indicato per le immagini a 3 canali di 8 bit, ma supporta anche profondità differenti. Il numero di bit necessari dipende dal numero di pixel, con un fattore moltiplicativo di 24.

La compressione non introduce perdita, utilizzando RLE: questo metodo può eventualmente fallire in caso di pattern disomogenei. 

\subsection{GIF}
GIF permette la sovrapposizione tra immagini, attivando la trasparenza uniforme 0-1.

La compressione è senza perdita, ma la limitazione di 256 colori quantizza pesantemente il numero di combinazioni a disposizione.

\subsection{TIFF}
TIFF gestisce immagini con contenuti ad altissime frequenze, sfruttando algoritmi che cercano ripetizioni di pattern a discapito delle dimensioni.

Permette di salvare caratteristiche aggiuntive memorizzando le informazioni colore in spazi diversi, definendo profili standard per allineare risposte di devices differenti.

 \subsection{PNG}
 PNG è una valida alternativa a GIF con diverse profondità colore, fino a 48 bit. Un ulteriore passaggio è l'alpha-channel per gestire la trasparenza, estendendo 0-1 a più livelli. 
 
 \subsection{JPEG}